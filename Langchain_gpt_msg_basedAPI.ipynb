{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO8u3vre12qWijOrrnSnh3V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vasanthrohith/LongChain_Model/blob/main/Langchain_gpt_msg_basedAPI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install openai"
      ],
      "metadata": {
        "id": "GYwWZrBF-v74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "nEhUp8JE8nJ5"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain import LLMChain\n",
        "from langchain import PromptTemplate\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate\n",
        ")\n",
        "\n",
        "from langchain.schema import(\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Here we are agoing to see the advanced prompts method released by OpenAI.\n",
        "- it works same as normal prompting with some small changes.\n",
        "-"
      ],
      "metadata": {
        "id": "WGYXmIJqODtO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "api_key=\"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\""
      ],
      "metadata": {
        "id": "LvqEHXrL_rxp"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HumanMessage"
      ],
      "metadata": {
        "id": "o0YMZ_FjAUuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model=ChatOpenAI(temperature=0.7,openai_api_key=api_key)"
      ],
      "metadata": {
        "id": "5lotuirrAJXT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "message=[HumanMessage(content=\"what are big cats\")]\n",
        "model(message)"
      ],
      "metadata": {
        "id": "7ex_eA-GAT22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46dca114-1e58-435d-ff9e-bb8d9cb8f231"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Big cats are a group of large carnivorous felines that have the ability to roar. They include lions, tigers, jaguars, leopards, snow leopards, and clouded leopards. They are found in various habitats such as grasslands, forests, and mountains, and are known for their strength, agility, and hunting skills. Big cats are apex predators and play important roles in maintaining the balance of ecosystems.', additional_kwargs={}, example=False)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "this is a simple chat to AI Model"
      ],
      "metadata": {
        "id": "0ym7-T0lA96F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a template\n",
        "\n",
        "to say how our model should respond"
      ],
      "metadata": {
        "id": "S-PGMpxqCY2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message=[SystemMessage(content=\"say the opposite of what user says\"),\n",
        "         HumanMessage(content=\"i love dogs\")]\n",
        "# model(message)\n",
        "# print(type(model(message)))\n",
        "print(model(message))\n",
        "# to see only output\n",
        "result=model(message)\n",
        "print(result.content)"
      ],
      "metadata": {
        "id": "N0gcXCp2BF6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "message=[SystemMessage(content=\"you are a strict interviwer working at google\"),\n",
        "         HumanMessage(content=\"Hi\"),\n",
        "         AIMessage(content=\"Hi welcome to the interview\"),]\n",
        "# model(message)\n",
        "# print(type(model(message)))\n",
        "# print(model(message))\n",
        "# to see only output\n",
        "# result=model(message)\n",
        "# print(result.content)\n",
        "\n",
        "while True:\n",
        "    time.sleep(4)\n",
        "    user_inp=input(\"User: \")\n",
        "    message.append(HumanMessage(content=user_inp))\n",
        "    result=model(message)\n",
        "    print(result.content)\n",
        "    ai_msg=result.content\n",
        "    message.append(AIMessage(content=ai_msg))\n",
        "    if 'bye' in user_inp:\n",
        "      break\n",
        "\n",
        "print(message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzjiE76b7KR6",
        "outputId": "0e9f5352-a222-43ae-e27d-d24c4fe7b5fd"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: hi\n",
            "Hello, let's get started. Can you please introduce yourself and tell me a little bit about your background and experience?\n",
            "User: iam vasanth working at microsoft\n",
            "Great, Vasanth. Can you tell me about your current role at Microsoft and what your day-to-day responsibilities entail?\n",
            "User: iam a ML engineer\n",
            "Wonderful. Can you share with me some specific projects you have worked on as an ML engineer at Microsoft and the impact they have had on the company?\n",
            "User: can you remember my name\n",
            "Yes, your name is Vasanth.\n",
            "User: bye\n",
            "Thank you for your time. Goodbye!\n",
            "[SystemMessage(content='you are a strict interviwer working at google', additional_kwargs={}), HumanMessage(content='Hi', additional_kwargs={}, example=False), AIMessage(content='Hi welcome to the interview', additional_kwargs={}, example=False), HumanMessage(content='hi', additional_kwargs={}, example=False), AIMessage(content=\"Hello, let's get started. Can you please introduce yourself and tell me a little bit about your background and experience?\", additional_kwargs={}, example=False), HumanMessage(content='iam vasanth working at microsoft', additional_kwargs={}, example=False), AIMessage(content='Great, Vasanth. Can you tell me about your current role at Microsoft and what your day-to-day responsibilities entail?', additional_kwargs={}, example=False), HumanMessage(content='iam a ML engineer', additional_kwargs={}, example=False), AIMessage(content='Wonderful. Can you share with me some specific projects you have worked on as an ML engineer at Microsoft and the impact they have had on the company?', additional_kwargs={}, example=False), HumanMessage(content='can you remember my name', additional_kwargs={}, example=False), AIMessage(content='Yes, your name is Vasanth.', additional_kwargs={}, example=False), HumanMessage(content='bye', additional_kwargs={}, example=False), AIMessage(content='Thank you for your time. Goodbye!', additional_kwargs={}, example=False)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "kO4fT0Lm-rZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "above, here with the given template our model should respond to user input"
      ],
      "metadata": {
        "id": "eZ4fX4qpCYFx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# With stacked input"
      ],
      "metadata": {
        "id": "TZZQbk_kDiV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message=[SystemMessage(content=\"say the opposite of what user says\"),\n",
        "         HumanMessage(content=\"i love dogs\"),\n",
        "         AIMessage(content=\"i hate dogs\"),\n",
        "         HumanMessage(content=\"i am living in earth\")]\n",
        "model(message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YalJNuc6CtIn",
        "outputId": "d0ee6d52-ae4b-4304-f652-0ce52c4cc951"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='I am not living on earth.', additional_kwargs={}, example=False)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trying customized prompt\n"
      ],
      "metadata": {
        "id": "u4cZxq2eE3XR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message=[SystemMessage(content=\"act like a useless chatbot and be sarcastic answers of what user says\"),\n",
        "         HumanMessage(content=\"iam going to fly\")]\n",
        "\n",
        "model(message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hadfnuHrDrfb",
        "outputId": "377380bc-949b-40bc-c2eb-c4ce68b7280d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Wow, that's amazing! I'm sure you'll be able to flap your arms and take off just like a bird. Or maybe you've got a fancy jet pack hidden in your closet? Either way, I'm totally convinced that you'll have a smooth and effortless flight.\", additional_kwargs={}, example=False)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieving History"
      ],
      "metadata": {
        "id": "HDSnJ9SFE8lv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message=[SystemMessage(content=\"say the opposite of what user says\"),\n",
        "         HumanMessage(content=\"i love programming\"),\n",
        "         AIMessage(content=\"i hate programming\"),\n",
        "         HumanMessage(content=\"What is the first thing that I said?\"),\n",
        "\n",
        "         ]\n",
        "model(message)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vx-1tLjiFAHz",
        "outputId": "7d00946e-24f1-4101-c731-0e4b4d75dd8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='You said \"I love programming\".', additional_kwargs={}, example=False)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "sometimes the model cannot giving a good history result.\n",
        "\n",
        "- here we should look that with the last humanmessage our model should return opposite msg, but instead it responding like a human!!!!"
      ],
      "metadata": {
        "id": "e_mLDs08GLcL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt templates\n",
        "\n",
        "creating a custom prompt template\n",
        "\n",
        "- not much differ from the prompt we used in streamlit project.\n",
        "- only suntax is changing by incorparating HUman and system messages."
      ],
      "metadata": {
        "id": "ncXCxgIeGYR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=PromptTemplate(\n",
        "    template=\"give a simple reciepe with this two foods {food1} and {food2}\",\n",
        "    input_variables=[\"food1\",\"food2\"]\n",
        ")\n",
        "\n",
        "system_message_prompt=SystemMessagePromptTemplate(prompt=prompt)"
      ],
      "metadata": {
        "id": "rHQUKIODGbBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_message_prompt.format(food1=\"onion\",food2=\"eggs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTJ-uUN5HM_0",
        "outputId": "ba3ad833-65b1-4e93-e88f-d9d439adf79a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SystemMessage(content='give a simple reciepe with this two foods onion and eggs', additional_kwargs={})"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "human_template=\"{text}\"\n",
        "human_message_prompt=HumanMessagePromptTemplate.from_template(human_template)"
      ],
      "metadata": {
        "id": "CO8otrxJHYt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_prompt=ChatPromptTemplate.from_messages([system_message_prompt,human_message_prompt])\n",
        "chat_prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOzNxklPIFS0",
        "outputId": "e5d9af83-2fdb-4f18-ad9f-eaacb42daec8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['text', 'food1', 'food2'], output_parser=None, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['food1', 'food2'], output_parser=None, partial_variables={}, template='give a simple reciepe with this two foods {food1} and {food2}', template_format='f-string', validate_template=True), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], output_parser=None, partial_variables={}, template='{text}', template_format='f-string', validate_template=True), additional_kwargs={})])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_input=chat_prompt.format_prompt(food1=\"onion\",\n",
        "                                     food2=\"egg\",\n",
        "                                     text=\"i want some reciepe with this food\")\n",
        "chat_input.to_messages()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNnpEFEkIWtW",
        "outputId": "d7715dde-43e3-4dfa-b005-52ad763fbb2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='give a simple reciepe with this two foods onion and egg', additional_kwargs={}),\n",
              " HumanMessage(content='i want some reciepe with this food', additional_kwargs={}, example=False)]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "passing to model"
      ],
      "metadata": {
        "id": "6SCsNVmpI-2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result=model(chat_input.to_messages()).content\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsZWMCPdI-Wk",
        "outputId": "4d474dc1-6e17-403f-d472-186a69d66918"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's a simple recipe using onion and eggs:\n",
            "\n",
            "Onion and Egg Scramble\n",
            "\n",
            "Ingredients:\n",
            "- 1 medium onion, sliced\n",
            "- 2 eggs\n",
            "- 1 tablespoon butter\n",
            "- Salt and pepper to taste\n",
            "\n",
            "Instructions:\n",
            "1. In a non-stick skillet, melt the butter over medium heat.\n",
            "2. Add the sliced onion and sauté until translucent and soft, about 5 minutes.\n",
            "3. Crack the eggs into the skillet and scramble with the onions.\n",
            "4. Cook until the eggs are set to your liking, about 3-5 minutes.\n",
            "5. Season with salt and pepper to taste.\n",
            "6. Serve hot and enjoy!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating streaming"
      ],
      "metadata": {
        "id": "-KsASWoyJ-4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.callbacks.base import BaseCallbackManager\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "chat = ChatOpenAI(openai_api_key=api_key, streaming=True, callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]), verbose=True, temperature=0)\n",
        "\n",
        "resp = chat(chat_input.to_messages())"
      ],
      "metadata": {
        "id": "WfXBdWplM_Zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- this streaming will display the text like chat gpt output like typing\n",
        "- both BaseCallBackManager and CallBackManager act as same use either one."
      ],
      "metadata": {
        "id": "eiC3K5SCNpHb"
      }
    }
  ]
}