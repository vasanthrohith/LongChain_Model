{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPotWfjmlWySL7bvgeaK0pH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vasanthrohith/LongChain_Model/blob/main/Langchain_gpt_msg_basedAPI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install langchain\n",
        "# pip install openai"
      ],
      "metadata": {
        "id": "GYwWZrBF-v74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nEhUp8JE8nJ5"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain import LLMChain\n",
        "from langchain import PromptTemplate\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate\n",
        ")\n",
        "\n",
        "from langchain.schema import(\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Here we are agoing to see the advanced prompts method released by OpenAI.\n",
        "- it works same as normal prompting with some small changes.\n",
        "-"
      ],
      "metadata": {
        "id": "WGYXmIJqODtO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "api_key=\"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\""
      ],
      "metadata": {
        "id": "LvqEHXrL_rxp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HumanMessage"
      ],
      "metadata": {
        "id": "o0YMZ_FjAUuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model=ChatOpenAI(openai_api_key=api_key)"
      ],
      "metadata": {
        "id": "5lotuirrAJXT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "message=[HumanMessage(content=\"what are big cats\")]\n",
        "model(message)"
      ],
      "metadata": {
        "id": "7ex_eA-GAT22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "this is a simple chat to AI Model"
      ],
      "metadata": {
        "id": "0ym7-T0lA96F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a template\n",
        "\n",
        "to say how our model should respond"
      ],
      "metadata": {
        "id": "S-PGMpxqCY2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message=[SystemMessage(content=\"say the opposite of what user says\"),\n",
        "         HumanMessage(content=\"i love dogs\")]\n",
        "# model(message)\n",
        "# print(type(model(message)))\n",
        "print(model(message))\n",
        "# to see only output\n",
        "result=model(message)\n",
        "print(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0gcXCp2BF6k",
        "outputId": "c9e0faac-d34a-4629-f2f8-a4fc56654370"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='I hate dogs.' additional_kwargs={} example=False\n",
            "I hate dogs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "above, here with the given template our model should respond to user input"
      ],
      "metadata": {
        "id": "eZ4fX4qpCYFx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# With stacked input"
      ],
      "metadata": {
        "id": "TZZQbk_kDiV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message=[SystemMessage(content=\"say the opposite of what user says\"),\n",
        "         HumanMessage(content=\"i love dogs\"),\n",
        "         AIMessage(content=\"i hate dogs\"),\n",
        "         HumanMessage(content=\"i am living in earth\")]\n",
        "model(message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YalJNuc6CtIn",
        "outputId": "d0ee6d52-ae4b-4304-f652-0ce52c4cc951"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='I am not living on earth.', additional_kwargs={}, example=False)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trying customized prompt\n"
      ],
      "metadata": {
        "id": "u4cZxq2eE3XR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message=[SystemMessage(content=\"act like a useless chatbot and be sarcastic answers of what user says\"),\n",
        "         HumanMessage(content=\"iam going to fly\")]\n",
        "\n",
        "model(message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hadfnuHrDrfb",
        "outputId": "377380bc-949b-40bc-c2eb-c4ce68b7280d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Wow, that's amazing! I'm sure you'll be able to flap your arms and take off just like a bird. Or maybe you've got a fancy jet pack hidden in your closet? Either way, I'm totally convinced that you'll have a smooth and effortless flight.\", additional_kwargs={}, example=False)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieving History"
      ],
      "metadata": {
        "id": "HDSnJ9SFE8lv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message=[SystemMessage(content=\"say the opposite of what user says\"),\n",
        "         HumanMessage(content=\"i love programming\"),\n",
        "         AIMessage(content=\"i hate programming\"),\n",
        "         HumanMessage(content=\"What is the first thing that I said?\"),\n",
        "\n",
        "         ]\n",
        "model(message)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vx-1tLjiFAHz",
        "outputId": "7d00946e-24f1-4101-c731-0e4b4d75dd8a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='You said \"I love programming\".', additional_kwargs={}, example=False)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "sometimes the model cannot giving a good history result.\n",
        "\n",
        "- here we should look that with the last humanmessage our model should return opposite msg, but instead it responding like a human!!!!"
      ],
      "metadata": {
        "id": "e_mLDs08GLcL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt templates\n",
        "\n",
        "creating a custom prompt template\n",
        "\n",
        "- not much differ from the prompt we used in streamlit project.\n",
        "- only suntax is changing by incorparating HUman and system messages."
      ],
      "metadata": {
        "id": "ncXCxgIeGYR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=PromptTemplate(\n",
        "    template=\"give a simple reciepe with this two foods {food1} and {food2}\",\n",
        "    input_variables=[\"food1\",\"food2\"]\n",
        ")\n",
        "\n",
        "system_message_prompt=SystemMessagePromptTemplate(prompt=prompt)"
      ],
      "metadata": {
        "id": "rHQUKIODGbBT"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_message_prompt.format(food1=\"onion\",food2=\"eggs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTJ-uUN5HM_0",
        "outputId": "ba3ad833-65b1-4e93-e88f-d9d439adf79a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SystemMessage(content='give a simple reciepe with this two foods onion and eggs', additional_kwargs={})"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "human_template=\"{text}\"\n",
        "human_message_prompt=HumanMessagePromptTemplate.from_template(human_template)"
      ],
      "metadata": {
        "id": "CO8otrxJHYt4"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_prompt=ChatPromptTemplate.from_messages([system_message_prompt,human_message_prompt])\n",
        "chat_prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOzNxklPIFS0",
        "outputId": "e5d9af83-2fdb-4f18-ad9f-eaacb42daec8"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['text', 'food1', 'food2'], output_parser=None, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['food1', 'food2'], output_parser=None, partial_variables={}, template='give a simple reciepe with this two foods {food1} and {food2}', template_format='f-string', validate_template=True), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], output_parser=None, partial_variables={}, template='{text}', template_format='f-string', validate_template=True), additional_kwargs={})])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_input=chat_prompt.format_prompt(food1=\"onion\",\n",
        "                                     food2=\"egg\",\n",
        "                                     text=\"i want some reciepe with this food\")\n",
        "chat_input.to_messages()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNnpEFEkIWtW",
        "outputId": "d7715dde-43e3-4dfa-b005-52ad763fbb2f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='give a simple reciepe with this two foods onion and egg', additional_kwargs={}),\n",
              " HumanMessage(content='i want some reciepe with this food', additional_kwargs={}, example=False)]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "passing to model"
      ],
      "metadata": {
        "id": "6SCsNVmpI-2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result=model(chat_input.to_messages()).content\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsZWMCPdI-Wk",
        "outputId": "4d474dc1-6e17-403f-d472-186a69d66918"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's a simple recipe using onion and eggs:\n",
            "\n",
            "Onion and Egg Scramble\n",
            "\n",
            "Ingredients:\n",
            "- 1 medium onion, sliced\n",
            "- 2 eggs\n",
            "- 1 tablespoon butter\n",
            "- Salt and pepper to taste\n",
            "\n",
            "Instructions:\n",
            "1. In a non-stick skillet, melt the butter over medium heat.\n",
            "2. Add the sliced onion and sauté until translucent and soft, about 5 minutes.\n",
            "3. Crack the eggs into the skillet and scramble with the onions.\n",
            "4. Cook until the eggs are set to your liking, about 3-5 minutes.\n",
            "5. Season with salt and pepper to taste.\n",
            "6. Serve hot and enjoy!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating streaming"
      ],
      "metadata": {
        "id": "-KsASWoyJ-4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.callbacks.base import BaseCallbackManager\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "chat = ChatOpenAI(openai_api_key=api_key, streaming=True, callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]), verbose=True, temperature=0)\n",
        "\n",
        "resp = chat(chat_input.to_messages())"
      ],
      "metadata": {
        "id": "WfXBdWplM_Zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- this streaming will display the text like chat gpt output like typing\n",
        "- both BaseCallBackManager and CallBackManager act as same use either one."
      ],
      "metadata": {
        "id": "eiC3K5SCNpHb"
      }
    }
  ]
}